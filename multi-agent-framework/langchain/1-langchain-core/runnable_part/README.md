# Runnables â€” LangChain çš„æ ¸å¿ƒåè®®

> `Runnable` æ˜¯ LangChain çš„**ç»å¯¹æ ¸å¿ƒ**ï¼Œ**ä¸”æ²¡æœ‰ä¹‹ä¸€**ã€‚
> æ— è®ºæ˜¯ Modelã€Toolã€Prompt è¿˜æ˜¯ Parserï¼Œæ‰€æœ‰ç»„ä»¶éƒ½å®ç°äº† `Runnable` æ¥å£ã€‚
> LangChain è¿™æ ·è®¾è®¡ï¼Œæˆ‘ä¼°è®¡æ˜¯æœ‰ä»¥ä¸‹åŸå› 
>
> 1ï¼Œä¸ºäº†ä¸åŒçš„æƒ…å†µä¸‹ï¼Œä¾æ—§èƒ½å®ç°ç»Ÿä¸€çš„è°ƒç”¨æ–¹æ³•ï¼Œæ˜¯é«˜åº¦æŠ½è±¡çš„è®¾è®¡ã€‚
>
> 2ï¼Œè¿™é‡Œåæ§½ä¸€ä¸‹ï¼Œæ„Ÿè§‰æ˜¯æ²¡æœ‰å¿…è¦çš„ä¸œè¥¿ï¼Œè®¾è®¡æœ‰ç‚¹è¿‡äºå¤æ‚

---

## ğŸ“¦ `runnables/base.py` æ ¸å¿ƒç±»

```
Runnable (ABC, Generic[Input, Output])        â† åŸºç±»
    â”‚
    â”œâ”€â”€ RunnableSerializable                  â† å¯åºåˆ—åŒ–çš„ Runnable
    â”‚
    â”œâ”€â”€ RunnableSequence                      â† ä¸²è¡Œé“¾ï¼ˆA | B | Cï¼‰
    â”‚
    â”œâ”€â”€ RunnableParallel                      â† å¹¶è¡Œé“¾ï¼ˆ{"a": A, "b": B}ï¼‰
    â”‚
    â”œâ”€â”€ RunnableGenerator                     â† ç”Ÿæˆå™¨å‡½æ•°åŒ…è£…å™¨
    â”‚
    â””â”€â”€ RunnableLambda                        â† æ™®é€šå‡½æ•°åŒ…è£…å™¨
```



---

## ğŸŒŸ ç¬¬ä¸€éƒ¨åˆ†ï¼šRunnable æ¥å£å®šä¹‰

### æ ¸å¿ƒæ–¹æ³•ï¼ˆ4 ç±»ï¼‰

| åˆ†ç±» | æ–¹æ³• | è¯´æ˜ |
| :--- | :--- | :--- |
| **æ‰§è¡Œ** | `invoke` / `ainvoke` | å•æ¬¡è°ƒç”¨ï¼ˆåŒæ­¥/å¼‚æ­¥ï¼‰ |
| | `stream` / `astream` | æµå¼è¾“å‡º |
| | `batch` / `abatch` | æ‰¹é‡å¹¶å‘ |
| | `transform` / `atransform` | æµå¼è¾“å…¥ â†’ æµå¼è¾“å‡º |
| **ç»„åˆ** | `__or__` (`\|`) | ä¸²è¡Œç»„åˆï¼š`A \| B \| C` â†’ `RunnableSequence` |
| | `pipe()` | åŒä¸Šï¼Œæ–¹æ³•è°ƒç”¨ç‰ˆ |
| | `pick()` | ä» dict è¾“å‡ºä¸­é€‰ key |
| | `assign()` | ç»™ dict è¾“å‡ºæ·»åŠ æ–° key |
| **è£…é¥°** | `bind()` | ç»‘å®šé»˜è®¤å‚æ•°ï¼ˆAgent ç»‘å®šå·¥å…·çš„åŸºç¡€ï¼‰ |
| | `with_config()` | ç»‘å®šè¿è¡Œæ—¶é…ç½® |
| | `with_retry()` | å¤±è´¥è‡ªåŠ¨é‡è¯• |
| | `with_fallbacks()` | å¤±è´¥åˆ‡æ¢å¤‡ç”¨æ–¹æ¡ˆ |
| | `with_listeners()` | æ·»åŠ ç”Ÿå‘½å‘¨æœŸé’©å­ |
| **å†…çœ** | `input_schema` / `output_schema` | è·å–è¾“å…¥/è¾“å‡ºçš„ Pydantic Schema |
| | `get_graph()` | è·å–å›¾ç»“æ„ï¼ˆå¯è§†åŒ–ç”¨ï¼‰ |

### è®¾è®¡åˆè¡·

LangChain æ—©æœŸå„ç»„ä»¶è°ƒç”¨æ–¹å¼ä¸ç»Ÿä¸€
`Runnable` çš„å‡ºç°å°†**æ‰€æœ‰ç»„ä»¶ç»Ÿä¸€ä¸ºåŒä¸€å¥—æ–¹æ¡ˆ**ï¼Œè§£å†³äº†ä»¥ä¸‹é—®é¢˜ï¼š

Runnable çš„æºç ä¸ºä»¥ä¸‹ï¼Œä½†æ˜¯å¤ªé•¿äº†ï¼Œè¿™é‡ŒæŒ‰ä¸‹ä¸è¡¨ï¼Œåç»­å†è¯´

```python
class Runnable(ABC, Generic[Input, Output]):
    """A unit of work that can be invoked, batched, streamed, transformed and composed.

    Key Methods
    ===========

    - `invoke`/`ainvoke`: Transforms a single input into an output.
    - `batch`/`abatch`: Efficiently transforms multiple inputs into outputs.
    - `stream`/`astream`: Streams output from a single input as it's produced.
    - `astream_log`: Streams output and selected intermediate results from an
        input.
    name: str | None
    """The name of the `Runnable`. Used for debugging and tracing."""
```



1. **è°ƒç”¨æ–¹æ³•çš„ç»Ÿä¸€** â†’ ç»Ÿä¸€ `invoke`/`stream`/`batch`

2. **æ³›å‹æ¨æ–­**

   ```python
   @property
       def InputType(self) -> type[Input]:  # noqa: N802
           """Input type.
   
           The type of input this `Runnable` accepts specified as a type annotation.
   
           Raises:
               TypeError: If the input type cannot be inferred.
           """
           # First loop through all parent classes and if any of them is
           # a Pydantic model, we will pick up the generic parameterization
           # from that model via the __pydantic_generic_metadata__ attribute.
           for base in self.__class__.mro():
               if hasattr(base, "__pydantic_generic_metadata__"):
                   metadata = base.__pydantic_generic_metadata__
                   if (
                       "args" in metadata
                       and len(metadata["args"]) == _RUNNABLE_GENERIC_NUM_ARGS
                   ):
                       return cast("type[Input]", metadata["args"][0])
   
           # If we didn't find a Pydantic model in the parent classes,
           # then loop through __orig_bases__. This corresponds to
           # Runnables that are not pydantic models.
           for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
               type_args = get_args(cls)
               if type_args and len(type_args) == _RUNNABLE_GENERIC_NUM_ARGS:
                   return cast("type[Input]", type_args[0])
   
           msg = (
               f"Runnable {self.get_name()} doesn't have an inferable InputType. "
               "Override the InputType property to specify the input type."
           )
           raise TypeError(msg)
   
       @property
       def OutputType(self) -> type[Output]:  # noqa: N802
           """Output Type.
   
           The type of output this `Runnable` produces specified as a type annotation.
   
           Raises:
               TypeError: If the output type cannot be inferred.
           """
           # First loop through bases -- this will help generic
           # any pydantic models.
           for base in self.__class__.mro():
               if hasattr(base, "__pydantic_generic_metadata__"):
                   metadata = base.__pydantic_generic_metadata__
                   if (
                       "args" in metadata
                       and len(metadata["args"]) == _RUNNABLE_GENERIC_NUM_ARGS
                   ):
                       return cast("type[Output]", metadata["args"][1])
   
           for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
               type_args = get_args(cls)
               if type_args and len(type_args) == _RUNNABLE_GENERIC_NUM_ARGS:
                   return cast("type[Output]", type_args[1])
   
           msg = (
               f"Runnable {self.get_name()} doesn't have an inferable OutputType. "
               "Override the OutputType property to specify the output type."
           )
           raise TypeError(msg)
   ```

   

3. **ç»„åˆå¼çš„æ‰§è¡Œ** â†’ `|` å…¶åº•å±‚é‡å†™äº† `__or__` æ–¹æ³•

   ```python
   def __or__(
           self,
           other: Runnable[Any, Other]
           | Callable[[Iterator[Any]], Iterator[Other]]
           | Callable[[AsyncIterator[Any]], AsyncIterator[Other]]
           | Callable[[Any], Other]
           | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any],
       ) -> RunnableSerializable[Input, Other]:
           """Runnable "or" operator.
   
           Compose this `Runnable` with another object to create a
           `RunnableSequence`.
   
           Args:
               other: Another `Runnable` or a `Runnable`-like object.
   
           Returns:
               A new `Runnable`.
           """
           return RunnableSequence(self, coerce_to_runnable(other))
   ```

   è¿™ä½¿å¾—å°è£…å‡ºä¸€ä¸ªSequenceåºåˆ—ï¼Œå°†ä¸Šä¸€æ­¥çš„ç»“æœä½œä¸ºä¸‹ä¸€æ­¥ç»„ä»¶çš„è¾“å‡ºï¼Œå½“å½¢æˆäº† `langchain` çš„ç»„ä»¶ä¹‹æ—¶ä¾‹å¦‚ä»¥ä¸‹ä¾‹å­ã€‚

   ```python
   chain = prompt | model    # è¿™é‡Œå‡è®¾ prompt ä¸ºchatpromptä¹‹ç±»çš„å¯¹è±¡çš„æ—¶å€™ï¼Œ ç”±äº Runnable é‡å†™äº† __or__ é­”æœ¯æ–¹æ³•
   chain = prompt.__or__(model) # é‚£ä¹ˆä»¥ä¸Šçš„åŠ¨ä½œå°±å˜æˆäº†è¿™æ ·å­ï¼Œä½¿å¾—å…¶è¿”å›äº† RunnableSequence å¯¹è±¡ï¼Œå½“éœ€è¦ä¸²è¡Œå…¶ä»–ç»„ä»¶çš„æ—¶å€™ï¼Œé‡å¤ä»¥ä¸Šçš„æ“ä½œå³å¯
   ```

   **è¿™ä¾¿æ˜¯`langchain` æœ€åˆä¸²è”ç»„ä»¶çš„æ ¸å¿ƒæ–¹å¼ã€‚**

   å½“ç„¶è¿™é‡Œåˆå‡ºç°äº†ä¸€ä¸ªç¼ºç‚¹ï¼Œè¿™å°±è¦å›åˆ° `Agent` çš„å®šä¹‰ä¸Šå»äº†ã€‚

   ä»€ä¹ˆæ˜¯ `Agent` , å³ ***An LLM agent runs tools in a loop to achieve a goal***

   key point is ***the loop*** ä½†æ˜¯å…¶ä¸²è¡Œçš„æ–¹å¼æ„å‘³ç€è¿™æ— æ³•è¿›è¡Œè‡ªæ£€å’Œå¾ªç¯ï¼Œè¿™å°±ä¸ç¬¦åˆå…¶å®šä¹‰

   å› æ­¤ `langchain` ä¾¿æ¨å‡ºäº† `langgraph`  ä»¥åŠåç»­çš„å¤§æ”¹ç‰ˆï¼Œ å½“ç„¶å°±è¿™æ˜¯å…¶ä»–æ¨¡å—è¦è¯´çš„ä¸œè¥¿äº†ã€‚ 

   ** **

4. **å¼‚æ­¥/æµå¼é‡å¤å†™** â†’ åŸºç±»æä¾›é»˜è®¤å®ç°

5. **ç±»å‹ä¸é€æ˜** â†’ `input_schema`/`output_schema` è‡ªåŠ¨æ¨æ–­

---

## ğŸŒŸç¬¬äºŒéƒ¨åˆ†ï¼šä¸€åˆ‡éƒ½æ˜¯Serializableä¹‹RunnableSerializable

~~~python
class RunnableSerializable(Serializable, Runnable[Input, Output]):
    """Runnable that can be serialized to JSON."""

    name: str | None = None
    """The name of the `Runnable`.

    Used for debugging and tracing.
    """

    model_config = ConfigDict(
        # Suppress warnings from pydantic protected namespaces
        # (e.g., `model_`)
        protected_namespaces=(),
    )

    @override
    def to_json(self) -> SerializedConstructor | SerializedNotImplemented:
        """Serialize the `Runnable` to JSON.

        Returns:
            A JSON-serializable representation of the `Runnable`.

        """
        dumped = super().to_json()
        with contextlib.suppress(Exception):
            dumped["name"] = self.get_name()
        return dumped

    def configurable_fields(
        self, **kwargs: AnyConfigurableField
    ) -> RunnableSerializable[Input, Output]:
        """Configure particular `Runnable` fields at runtime.

        Args:
            **kwargs: A dictionary of `ConfigurableField` instances to configure.

        Raises:
            ValueError: If a configuration key is not found in the `Runnable`.

        Returns:
            A new `Runnable` with the fields configured.

        !!! example

            ```python
            from langchain_core.runnables import ConfigurableField
            from langchain_openai import ChatOpenAI

            model = ChatOpenAI(max_tokens=20).configurable_fields(
                max_tokens=ConfigurableField(
                    id="output_token_number",
                    name="Max tokens in the output",
                    description="The maximum number of tokens in the output",
                )
            )

            # max_tokens = 20
            print(
                "max_tokens_20: ", model.invoke("tell me something about chess").content
            )

            # max_tokens = 200
            print(
                "max_tokens_200: ",
                model.with_config(configurable={"output_token_number": 200})
                .invoke("tell me something about chess")
                .content,
            )
            ```
        """
        # Import locally to prevent circular import
        from langchain_core.runnables.configurable import (  # noqa: PLC0415
            RunnableConfigurableFields,
        )

        model_fields = type(self).model_fields
        for key in kwargs:
            if key not in model_fields:
                msg = (
                    f"Configuration key {key} not found in {self}: "
                    f"available keys are {model_fields.keys()}"
                )
                raise ValueError(msg)

        return RunnableConfigurableFields(default=self, fields=kwargs)

    def configurable_alternatives(
        self,
        which: ConfigurableField,
        *,
        default_key: str = "default",
        prefix_keys: bool = False,
        **kwargs: Runnable[Input, Output] | Callable[[], Runnable[Input, Output]],
    ) -> RunnableSerializable[Input, Output]:
        """Configure alternatives for `Runnable` objects that can be set at runtime.

        Args:
            which: The `ConfigurableField` instance that will be used to select the
                alternative.
            default_key: The default key to use if no alternative is selected.
            prefix_keys: Whether to prefix the keys with the `ConfigurableField` id.
            **kwargs: A dictionary of keys to `Runnable` instances or callables that
                return `Runnable` instances.

        Returns:
            A new `Runnable` with the alternatives configured.

        !!! example

            ```python
            from langchain_anthropic import ChatAnthropic
            from langchain_core.runnables.utils import ConfigurableField
            from langchain_openai import ChatOpenAI

            model = ChatAnthropic(
                model_name="claude-sonnet-4-5-20250929"
            ).configurable_alternatives(
                ConfigurableField(id="llm"),
                default_key="anthropic",
                openai=ChatOpenAI(),
            )

            # uses the default model ChatAnthropic
            print(model.invoke("which organization created you?").content)

            # uses ChatOpenAI
            print(
                model.with_config(configurable={"llm": "openai"})
                .invoke("which organization created you?")
                .content
            )
            ```
        """
        # Import locally to prevent circular import
        from langchain_core.runnables.configurable import (  # noqa: PLC0415
            RunnableConfigurableAlternatives,
        )

        return RunnableConfigurableAlternatives(
            which=which,
            default=self,
            alternatives=kwargs,
            default_key=default_key,
            prefix_keys=prefix_keys,
        )

~~~

å…¶æ‰¿è½½çš„æ ¸å¿ƒåŠŸèƒ½å°±æ˜¯Serializeæ‰€æœ‰å¯Serializeçš„Runnableçš„å¯¹è±¡ï¼Œlangchainé‡å†™äº†Serializableï¼Œå¡«å……äº†å…³äºlcçš„ä¸€å †å±æ€§ï¼Œå¦‚ä¸‹

```python
    @property
    def lc_secrets(self) -> dict[str, str]:
        """A map of constructor argument names to secret ids.

        For example, `{"openai_api_key": "OPENAI_API_KEY"}`
        """
        return {}

    @property
    def lc_attributes(self) -> dict:
        """List of attribute names that should be included in the serialized kwargs.

        These attributes must be accepted by the constructor.

        Default is an empty dictionary.
        """
        return {}

    @classmethod
    def lc_id(cls) -> list[str]:
        """Return a unique identifier for this class for serialization purposes.

        The unique identifier is a list of strings that describes the path
        to the object.

        For example, for the class `langchain.llms.openai.OpenAI`, the id is
        `["langchain", "llms", "openai", "OpenAI"]`.
```

ç­‰ç­‰æ–¹æ³•ï¼Œåœ¨langchainä¸­ä¸‡ç‰©çš†å¯¹è±¡ï¼ŒåºŸè¯å…¶å®ï¼Œå¯¹è±¡å°±æœ‰ç‹¬ä¸€æ— äºŒçš„å±æ€§ã€‚

## ğŸŒŸ ç¬¬ä¸‰éƒ¨åˆ†ï¼šç»„åˆåºåˆ—

### ä¸€ï¼Œ`RunnableSequence` â€” ä¸²è¡Œé“¾

```python
chain = prompt | model | parser
# å†…éƒ¨ï¼šRunnableSequence(first=prompt, middle=[model], last=parser)
# æ‰§è¡Œï¼šprompt çš„è¾“å‡º â†’ model çš„è¾“å…¥ â†’ parser çš„è¾“å…¥
```

`|` æ“ä½œç¬¦å°±æ˜¯ `__or__` é‡è½½ï¼Œè¿”å›ä¸€ä¸ª `RunnableSequence` å¯¹è±¡ã€‚

å› æ­¤ï¼Œå½“Runnableå¯¹è±¡ä½¿ç”¨ `__or__` æ–¹æ³•çš„æ—¶å€™ï¼ŒRunnableå¯¹è±¡è‡ªå·±å°±å˜æˆäº†  `RunnableSequence`

```python
def __or__(
        self,
        other: Runnable[Any, Other]
        | Callable[[Iterator[Any]], Iterator[Other]]
        | Callable[[AsyncIterator[Any]], AsyncIterator[Other]]
        | Callable[[Any], Other]
        | Mapping[str, Runnable[Any, Other] | Callable[[Any], Other] | Any],
    ) -> RunnableSerializable[Input, Other]:
        """Runnable "or" operator.

        Compose this `Runnable` with another object to create a
        `RunnableSequence`.

        Args:
            other: Another `Runnable` or a `Runnable`-like object.

        Returns:
            A new `Runnable`.
        """
        return RunnableSequence(self, coerce_to_runnable(other))
```

è¿™é‡Œ`coerce_to_runnable` ä¼šæŠŠç±»Runnableçš„æ‰€æœ‰ç±»è½¬æˆRunnable, ä¹Ÿæ˜¯ä¸ºäº†ç»Ÿä¸€

### äºŒï¼Œ`RunnableParallel` â€” å¹¶è¡Œé“¾

å®˜æ–¹åœ¨æ³¨é‡Šä¸­å†™æ˜äº†

***  ***

***RunnableParallel is one of the two main composition primitives***

å˜›æ„æ€å‘¢ï¼Ÿ

å¤§ç™½è¯å°±æ˜¯ï¼ŒRunnalbeParallel æ˜¯éå¸¸é‡è¦ç»„åˆä»¶ä¹‹ä¸€ï¼Œå¦å¤–ä¸€ä¸ªæ˜¯å˜›å‘¢ï¼Œå°±æ˜¯ä¸Šé¢çš„RunnableSequenece 

åœ¨è¿™é‡Œè¯´ä¸‹è¿™ä¸¤ç§æ–¹å¼æœ‰ä½•ä¸åŒ

**æˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡äº†ï¼ŒRunnableSequence æ˜¯ Runnable è°ƒç”¨ or æ–¹æ³•åè¿”å›çš„ç»“æœï¼Œé‚£ä¹ˆ Sequence ç©¶ç«Ÿäº§ç”Ÿäº†ä¸€ä¸ªä»€ä¹ˆç»“æœå‘¢ï¼Ÿ**çœ‹ä¸‹ä»–çš„initæ–¹æ³•

```python
first: Runnable[Input, Any]
    """The first `Runnable` in the sequence."""
    middle: list[Runnable[Any, Any]] = Field(default_factory=list)
    """The middle `Runnable` in the sequence."""
    last: Runnable[Any, Output]
    """The last `Runnable` in the sequence."""

    def __init__(
        self,
        *steps: RunnableLike,
        name: str | None = None,
        first: Runnable[Any, Any] | None = None,
        middle: list[Runnable[Any, Any]] | None = None,
        last: Runnable[Any, Any] | None = None,
    ) -> None:
        """Create a new `RunnableSequence`.

        Args:
            steps: The steps to include in the sequence.
            name: The name of the `Runnable`.
            first: The first `Runnable` in the sequence.
            middle: The middle `Runnable` objects in the sequence.
            last: The last `Runnable` in the sequence.

        Raises:
            ValueError: If the sequence has less than 2 steps.
        """
        steps_flat: list[Runnable] = []
        if not steps and first is not None and last is not None:
            steps_flat = [first] + (middle or []) + [last]
        for step in steps:
            if isinstance(step, RunnableSequence):
                steps_flat.extend(step.steps)
            else:
                steps_flat.append(coerce_to_runnable(step))
        if len(steps_flat) < _RUNNABLE_SEQUENCE_MIN_STEPS:
            msg = (
                f"RunnableSequence must have at least {_RUNNABLE_SEQUENCE_MIN_STEPS} "
                f"steps, got {len(steps_flat)}"
            )
            raise ValueError(msg)
        super().__init__(
            first=steps_flat[0],
            middle=list(steps_flat[1:-1]),
            last=steps_flat[-1],
            name=name,
        )
```

è¿™é‡Œ RunnableSequence æ–¹æ³•ï¼Œå®šä¹‰äº†ä¸‰ä¸ªå‚æ•°ï¼Œfirstã€middleã€last é¦–å°¾å‚æ•°éƒ½æ˜¯ä¸€ä¸ª Runnable å¯¹è±¡ï¼Œä¸­é—´æ˜¯ä¸€ä¸ª list çš„ Runnable å¯¹è±¡ã€‚

å†ç»“åˆSequenceè¿™ä¸ªæ–¹æ³•åï¼Œæ˜¾è€Œæ˜“è§ï¼Œè¿™æ˜¯ä¸€ä¸ªé¡ºåºçš„é“¾æ¡ï¼Œä¸‹é¢å†çœ‹å…¶æ˜¯å¦‚ä½•æ‹¼æ¥çš„

```python
        steps_flat: list[Runnable] = []
        if not steps and first is not None and last is not None:
            steps_flat = [first] + (middle or []) + [last]
        for step in steps:
            if isinstance(step, RunnableSequence):
                steps_flat.extend(step.steps)
            else:
                steps_flat.append(coerce_to_runnable(step))
        if len(steps_flat) < _RUNNABLE_SEQUENCE_MIN_STEPS:
            msg = (
                f"RunnableSequence must have at least {_RUNNABLE_SEQUENCE_MIN_STEPS} "
                f"steps, got {len(steps_flat)}"
            )
            raise ValueError(msg)
        super().__init__(
            first=steps_flat[0],
            middle=list(steps_flat[1:-1]),
            last=steps_flat[-1],
            name=name,
        )
```



å…¶ä»–çš„ä¸èµ˜è¿°ï¼Œè¦æ³¨æ„ä¸€ç‚¹ï¼Œå°±æ˜¯å½“ï¼Œstepï¼Œå³ä¸­é—´çš„ä¸€å †å­˜åœ¨æ—¶ï¼Œç›´æ¥ä¼šç”¨extendæ–¹æ³•é‡æ„ä¸‹ï¼Œæœ€åè°ƒç”¨ pydantic å®Œæˆæ•´ä½“çš„éªŒè¯

ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRunnableParallel çš„æ„å»ºæ–¹å¼å°± å¤æ‚ä¸€ç‚¹ï¼Œå…¶è§„å®šäº†å½¢æˆæ–¹å¼æ˜¯ key-value çš„å½¢å¼ï¼Œå› æ­¤å…¶åˆå§‹åŒ–çš„æ—¶å€™

å½¢æˆäº†çš„å½¢å¼ï¼Œå…¶ä¸­ `coerce_to_runnable` æ˜¯ä¸€ä¸ªå¼ºåˆ¶è½¬æ¢çš„æ–¹æ³•

```python
steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
```



```python
class RunnableParallel(RunnableSerializable[Input, dict[str, Any]]):
    """Runnable that runs a mapping of `Runnable`s in parallel.

    Returns a mapping of their outputs.

    `RunnableParallel` is one of the two main composition primitives,
    alongside `RunnableSequence`. It invokes `Runnable`s concurrently, providing the
    same input to each.

    A `RunnableParallel` can be instantiated directly or by using a dict literal
    within a sequence.

    Here is a simple example that uses functions to illustrate the use of
    `RunnableParallel`:

        ```python
        from langchain_core.runnables import RunnableLambda


        def add_one(x: int) -> int:
            return x + 1


        def mul_two(x: int) -> int:
            return x * 2


        def mul_three(x: int) -> int:
            return x * 3


        runnable_1 = RunnableLambda(add_one)
        runnable_2 = RunnableLambda(mul_two)
        runnable_3 = RunnableLambda(mul_three)

        sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel
            "mul_two": runnable_2,
            "mul_three": runnable_3,
        }
        # Or equivalently:
        # sequence = runnable_1 | RunnableParallel(
        #     {"mul_two": runnable_2, "mul_three": runnable_3}
        # )
        # Also equivalently:
        # sequence = runnable_1 | RunnableParallel(
        #     mul_two=runnable_2,
        #     mul_three=runnable_3,
        # )

        sequence.invoke(1)
        await sequence.ainvoke(1)

        sequence.batch([1, 2, 3])
        await sequence.abatch([1, 2, 3])
```

~~~python
`RunnableParallel` makes it easy to run `Runnable`s in parallel. In the below
example, we simultaneously stream output from two different `Runnable` objects:

    ```python
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnableParallel
    from langchain_openai import ChatOpenAI

    model = ChatOpenAI()
    joke_chain = (
        ChatPromptTemplate.from_template("tell me a joke about {topic}") | model
    )
    poem_chain = (
        ChatPromptTemplate.from_template("write a 2-line poem about {topic}")
        | model
    )

    runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)

    # Display stream
    output = {key: "" for key, _ in runnable.output_schema()}
    for chunk in runnable.stream({"topic": "bear"}):
        for key in chunk:
            output[key] = output[key] + chunk[key].content
        print(output)  # noqa: T201
    ```


steps__: Mapping[str, Runnable[Input, Any]]

def __init__(
    self,
    steps__: Mapping[
        str,
        Runnable[Input, Any]
        | Callable[[Input], Any]
        | Mapping[str, Runnable[Input, Any] | Callable[[Input], Any]],
    ]
    | None = None,
    **kwargs: Runnable[Input, Any]
    | Callable[[Input], Any]
    | Mapping[str, Runnable[Input, Any] | Callable[[Input], Any]],
) -> None:
    """Create a `RunnableParallel`.

    Args:
        steps__: The steps to include.
        **kwargs: Additional steps to include.

    """
    merged = {**steps__} if steps__ is not None else {}
    merged.update(kwargs)
    super().__init__(
        steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
   )
~~~
`RunnableParallel` çš„åˆå§‹åŒ–è¿è¡Œæ˜¯åœ¨invokeé˜¶æ®µå®Œæˆçš„(ä¹Ÿæ˜¯åºŸè¯)ï¼Œå…¶å®éƒ½æ˜¯è¿™ä¸ªé˜¶æ®µè¿è¡Œçš„ï¼Œåªä¸è¿‡æ˜¯è¿™ä¸ªç‰¹æ®Šä¸€ç‚¹ï¼Œ

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå…¶åº• RunnableParallel åº•å±‚ç”¨äº†ä¸€ä¸ª ç»§æ‰¿äº† `ThreadPoolExecutor` çš„ Â· `ContextThreadPoolExecutor` 

å…¶åŒæ­¥æ–¹æ³•ç”¨åˆ°çš„æ˜¯çº¿ç¨‹æ± çš„æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿ç•™äº†ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œå…·ä½“æ˜¯æ€ä¹ˆåšåˆ°äº†å‘¢ï¼Ÿ





### ä¸‰ï¼Œ`RunnableGenerator` â€” ç”Ÿæˆå™¨åŒ…è£…å™¨

è¯¥éƒ¨åˆ†ä¸å¤ªé‡è¦ï¼Œä¸»è¦æ˜¯åŒ…è£…ä¸€ä¸ªåº•å±‚çš„è¿­ä»£å™¨äº†ï¼Œå…¶ä»–çš„æ— ä»–

```python
def stream_words(input):
    for word in input.split():
        yield word

streamer = RunnableGenerator(stream_words)  # æ”¯æŒæµå¼
```

---

### å››ï¼Œ`RunnableEach` â€” **Each è¿è¡Œå•å…ƒ**

è¯¥éƒ¨åˆ†åº•å±‚ç”¨åˆ°çš„æ˜¯asynioçš„gatheræ–¹æ³•ï¼Œç„¶åéå†`config`

```python
class RunnableEachBase(RunnableSerializable[list[Input], list[Output]]):
    """RunnableEachBase class.

    `Runnable` that calls another `Runnable` for each element of the input sequence.

    Use only if creating a new `RunnableEach` subclass with different `__init__`
    args.

    See documentation for `RunnableEach` for more details.

    """

    bound: Runnable[Input, Output]

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )

    @property
    @override
    def InputType(self) -> Any:
        return list[self.bound.InputType]  # type: ignore[name-defined]

    @override
    def get_input_schema(self, config: RunnableConfig | None = None) -> type[BaseModel]:
        return create_model_v2(
            self.get_name("Input"),
            root=(
                list[self.bound.get_input_schema(config)],  # type: ignore[misc]
                None,
            ),
            # create model needs access to appropriate type annotations to be
            # able to construct the Pydantic model.
            # When we create the model, we pass information about the namespace
            # where the model is being created, so the type annotations can
            # be resolved correctly as well.
            # self.__class__.__module__ handles the case when the Runnable is
            # being sub-classed in a different module.
            module_name=self.__class__.__module__,
        )

    @property
    @override
    def OutputType(self) -> type[list[Output]]:
        return list[self.bound.OutputType]  # type: ignore[name-defined]

    @override
    def get_output_schema(
        self, config: RunnableConfig | None = None
    ) -> type[BaseModel]:
        schema = self.bound.get_output_schema(config)
        return create_model_v2(
            self.get_name("Output"),
            root=list[schema],  # type: ignore[valid-type]
            # create model needs access to appropriate type annotations to be
            # able to construct the Pydantic model.
            # When we create the model, we pass information about the namespace
            # where the model is being created, so the type annotations can
            # be resolved correctly as well.
            # self.__class__.__module__ handles the case when the Runnable is
            # being sub-classed in a different module.
            module_name=self.__class__.__module__,
        )

    @property
    @override
    def config_specs(self) -> list[ConfigurableFieldSpec]:
        return self.bound.config_specs

    @override
    def get_graph(self, config: RunnableConfig | None = None) -> Graph:
        return self.bound.get_graph(config)

    @classmethod
    @override
    def is_lc_serializable(cls) -> bool:
        """Return `True` as this class is serializable."""
        return True

    @classmethod
    @override
    def get_lc_namespace(cls) -> list[str]:
        """Get the namespace of the LangChain object.

        Returns:
            `["langchain", "schema", "runnable"]`
        """
        return ["langchain", "schema", "runnable"]

    def _invoke(
        self,
        inputs: list[Input],
        run_manager: CallbackManagerForChainRun,
        config: RunnableConfig,
        **kwargs: Any,
    ) -> list[Output]:
        configs = [
            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs
        ]
        return self.bound.batch(inputs, configs, **kwargs)

    @override
    def invoke(
        self, input: list[Input], config: RunnableConfig | None = None, **kwargs: Any
    ) -> list[Output]:
        return self._call_with_config(self._invoke, input, config, **kwargs)

    async def _ainvoke(
        self,
        inputs: list[Input],
        run_manager: AsyncCallbackManagerForChainRun,
        config: RunnableConfig,
        **kwargs: Any,
    ) -> list[Output]:
        configs = [
            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs
        ]
        return await self.bound.abatch(inputs, configs, **kwargs)

    @override
    async def ainvoke(
        self, input: list[Input], config: RunnableConfig | None = None, **kwargs: Any
    ) -> list[Output]:
        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
```



## ğŸŒŸ ç¬¬å››éƒ¨åˆ†ï¼šaliment of Runnable

### `bind()` â€” Agent ç»‘å®šå·¥å…·çš„åŸºç¡€

```python
model_with_tools = model.bind_tools(tools)
# åº•å±‚å°±æ˜¯ bind()ï¼Œå°†å·¥å…· schema ä½œä¸ºé»˜è®¤å‚æ•°ç»‘å®šåˆ°æ¨¡å‹ä¸Š
```

### `with_retry()` + `with_fallbacks()` â€” å®¹é”™æœºåˆ¶

```python
safe_model = model.with_retry(stop_after_attempt=3)
safe_model = gpt4.with_fallbacks([gpt35, local_model])
```



---

## ğŸ”— ç›¸å…³æºç 

- `langchain_core/runnables/base.py` â€” `Runnable` åŠæ‰€æœ‰ç»„åˆåŸè¯­çš„å®šä¹‰
- `langchain_core/runnables/config.py` â€” `RunnableConfig` è¿è¡Œæ—¶é…ç½®
